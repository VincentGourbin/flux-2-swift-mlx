# Klein 9B LoRA Training Configuration - TEST (5 steps)
# Quick validation that training works without full run

model:
  name: klein-9b
  quantization: int8  # Text encoder quantization (transformer always bf16)

lora:
  rank: 32
  alpha: 32.0
  target_layers: attention  # Includes Q, K, V, O projections

dataset:
  path: ./examples/tarot-style/train
  trigger_word: rwaite
  caption_format: txt

training:
  batch_size: 1
  gradient_accumulation: 1
  max_steps: 5
  warmup_steps: 1
  learning_rate: 1e-4
  weight_decay: 0.0001
  log_every_n_steps: 1
  eval_every_n_steps: 5

loss:
  timestep_sampling: balanced
  weighting: bell_shaped

checkpoints:
  output: ./output/klein9b-tarot-test
  save_every: 0  # No checkpoint for test
  keep_last: 1
  learning_curve: false
