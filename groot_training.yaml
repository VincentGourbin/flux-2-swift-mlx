# Groot LoRA Training Configuration
# Usage: flux2 train-lora --config groot_training.yaml
# (Feature not yet implemented - this file documents the current training parameters)

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
model:
  name: klein-4b                    # klein-4b, klein-9b, dev, schnell
  quantization: int8                # bf16, int8, int4

# =============================================================================
# LoRA CONFIGURATION
# =============================================================================
lora:
  rank: 32                          # LoRA rank (common: 4, 8, 16, 32, 64)
  alpha: 32.0                       # LoRA alpha (typically same as rank)
  # target_layers: attention        # Future: attention, mlp, all

# =============================================================================
# DATASET CONFIGURATION
# =============================================================================
dataset:
  path: /Users/vincent/Developpements/flux-2-swift-mlx/datasets/groot_training
  validation_path: /Users/vincent/Developpements/flux-2-swift-mlx/datasets/groot_validation
  trigger_word: "groot"             # Trigger word for the subject

# =============================================================================
# TRAINING PARAMETERS
# =============================================================================
training:
  batch_size: 1                     # Batch size per step
  gradient_accumulation: 1          # Effective batch = batch_size * gradient_accumulation
  max_steps: 3000                   # Maximum training steps
  warmup_steps: 100                 # Learning rate warmup steps
  learning_rate: 1.0e-4             # Peak learning rate
  weight_decay: 0.0001              # AdamW weight decay
  caption_dropout: 0.05             # Caption dropout rate (5%)
  max_grad_norm: 1.0                # Gradient clipping norm

# =============================================================================
# LOSS & TIMESTEP CONFIGURATION
# =============================================================================
loss:
  weighting: bell_shaped            # uniform, snr_weighted, bell_shaped (ostris)
  timestep_sampling: uniform        # uniform, logit_normal, sigmoid

# =============================================================================
# MEMORY OPTIMIZATION
# =============================================================================
memory:
  gradient_checkpointing: true      # Reduce memory at cost of speed
  cache_latents: true               # Pre-encode images with VAE
  cache_text_embeddings: false      # Pre-encode prompts (saves memory if true)
  cpu_offload: false                # Offload to CPU (slower but less VRAM)

  bucketing:
    enabled: true                   # Enable aspect ratio bucketing
    resolutions:                    # Base resolutions for bucket generation
      - 512
      - 768
      # - 1024                      # Disabled: 1024+ resolutions cause OOM on 96GB M3 Max

# =============================================================================
# CHECKPOINTING
# =============================================================================
checkpoints:
  output: /Users/vincent/Developpements/flux-2-swift-mlx/loras/groot_flux2swift.safetensors
  save_every: 250                   # Save checkpoint every N steps
  keep_last: 3                      # Keep only last N checkpoints (0 = keep all)

# =============================================================================
# VALIDATION
# =============================================================================
validation:
  prompt: "groot, dancing celebrating Christmas"
  every_n_steps: 250                # Generate validation image every N steps
  seed: 42                          # Fixed seed for reproducible validation
  # guidance: 4.0                   # CFG scale for validation (future)
  # steps: 25                       # Inference steps for validation (future)

# =============================================================================
# EMA (Exponential Moving Average)
# =============================================================================
ema:
  enabled: false                    # Use EMA weights for final model
  # decay: 0.9999                   # EMA decay rate (future)

# =============================================================================
# CURRENT CLI COMMAND (for reference)
# =============================================================================
# ./.build-xcode/Build/Products/Release/Flux2CLI train-lora \
#   /Users/vincent/Developpements/flux-2-swift-mlx/datasets/groot_training \
#   --validation-dataset /Users/vincent/Developpements/flux-2-swift-mlx/datasets/groot_validation \
#   --output ./loras/groot_flux2swift.safetensors \
#   --trigger-word "groot" \
#   --model klein-4b \
#   --quantization int8 \
#   --rank 32 \
#   --alpha 32.0 \
#   --batch-size 1 \
#   --max-steps 3000 \
#   --warmup-steps 100 \
#   --learning-rate 1e-4 \
#   --weight-decay 0.0001 \
#   --loss-weighting bell_shaped \
#   --timestep-sampling uniform \
#   --caption-dropout 0.05 \
#   --gradient-checkpointing \
#   --bucketing \
#   --bucket-resolutions "512,768,1024" \
#   --save-every-n-steps 250 \
#   --validation-prompt "groot, dancing celebrating Christmas" \
#   --validate-every-n-steps 250 \
#   --validation-seed 42 \
#   --no-ema \
#   --cache-latents
